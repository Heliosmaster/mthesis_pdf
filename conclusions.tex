\chapter{Conclusions and further developments} \label{chap:conclusions}

The goal of this thesis was firstly to investigate the opportunity of performing sparse matrix partitioning in an iterative fashion, devising a systematic approach for the selection of the information on the previous partitioning to be kept for an improvement on the quality of the solution. Secondly, our intent was also to explore the margins of improvement of the initial solution for the medium-grain model.

We were able to deal with these two research directions at the same time, applying basic principles to come up with simple schemes that could serve either purpose (with minor modifications). In the following sections, we summarize our findings and try to envision additional developments in these directions.

\section{Improvement of the initial solution} \label{sec:conclusions_po}

As already outlined in Section \ref{sec:mediumgrain}, for the medium-grain model the initial split of the matrix $A$ into $A_r$ and $A_c$ is of vital importance: our goal was to investigate whether the algorithm proposed by the authors in \cite{mediumgrain} was indeed a good choice and, if so, to what extent. To this purpose, we devised a number of different algorithms in Section \ref{sec:localview}, \ref{sec:hot_restart} and \ref{sec:is_vector}, which exploited different features of the underlying matrix. 

Among these proposed partition-oblivious heuristics, \verb|po_localview| (outlined in Section \ref{sec:localview}) and \verb|po_is| (described in Section \ref{sec:is_vector}) showed the best results during the preliminary testing. In the final tests, the behavior of these heuristics was slightly different: without the employment of the iterative refinement procedure, briefly described in Section \ref{sec:iter_refinement}, \verb|po_localview| produced the best results (7\% worse than \verb|medium-grain|), whereas with iterative refinement, \verb|po_is| is the heuristic that performs best (6\% higher communication volume than \verb|medium-grain|). 

The behavior of \verb|po_localview| was not surprising, as we already argued how it is a less refined version of the default algorithm for the medium-grain method, whereas the good result of \verb|po_is| confirm our expectations that the indipendent set formulation of Chapter \ref{chap:independent_set} is indeed worthwhile. Furthermore, it is interesting to observe that \verb|medium-grain| was the best algorithm, outperforming the partition-oblivious schemes devised in this thesis; this suggest us that it is indeed a valid algorithm to produce the initial $A_r$ and $A_c$ for the medium-grain method.

\section{Iterative partitioning} \label{sec:conclusions_pa}

It is reasonable to assume that the initial split of the matrix $A$ into $A_r$ and $A_c$ can be performed more efficiently with additional information, especially if the matrix $A$ has already been (bi)partitioned. The main feature of a partitioning we decided to preserve was the fact that a row/column was uncut: this means that the partitioner decided, at some point in the previous iteration, that it was convenient for the nonzeros of such row/column to be assigned to the same processor. It is then reasonable to devise heuristics which tend to give a preference for these uncut rows/columns, trying to keep them uncut also in the next iteration. Naturally, as it is not always possible to do (for example because we are not directly assigning to processors, but rather to $A_r$ and $A_c$), a more sophisticated approach is usually needed.

To this extent, a wide variety of heuristics has been devised: the partition-aware extension of the original algorithm, along with the heuristics that employ the Separated Block Diagonal structure of order 1 and 2 of the partitioned matrix $A$, quickly turned out to be far from effective. The other considered algorithms, instead, yielded more interesting results, especially with rectangular matrices.

Chapter \ref{chap:independent_set} was focused on the concept of independent set, and the experimental results suggest that this is indeed an interesting approach: our best heuristics (\verb|pa_localbest|, \verb|pa_is_1|, \verb|pa_is_3|) rely indeed on this concept, implicitly or explicitly. Even the simple concatenation of rows and columns in the priority vector $v$ can be intended as a special case of the ideas of Section \ref{sec:is_vector}: the set of rows (or columns) is as a matter of fact an independent set, although probably not of maximum cardinality. The improvement of the results of this algorithm with increasing rectangularity of the matrices is also to be seen in this perspective: the more one dimension is dominant, the more that set of indices has a cardinality closer to the maximum independent set, when computed on the graph constructed according to Section \ref{sec:is_graph}.

Without iterative refinement, described in Section \ref{sec:iter_refinement}, none of the proposed scheme outperforms \verb|medium-grain| for all test matrices: \verb|pa_localbest| and \verb|pa_simple| have a communication volume, on average, 4\% higher than \verb|medium-grain|.  Considering only the rectangular matrices in our test bed, instead, the results of these heuristics were better (albeit marginally) than \verb|medium-grain|: \verb|pa_is_3| had an average communication volume 2\% better than the default algorithm of the medium-grain method.

With the iterative refinement procedure, instead, the results are more encouraging, as the heuristic \verb|pa_localbest| was able to outperform \verb|medium-grain| even if slightly: the communication volume was, on average, 1\% lower than the reference value. Also the other heuristics benefited greatly from this procedure, and the quality of the final partitionings is indeed very close to the one achieved by \verb|medium-grain|. Also in the case of rectangular matrices the results are slightly better: \verb|pa_is_1| and \verb|pa_is_3| have a communication volume on average 4\% better than the reference value.

In general, the experimental results confirmed our theoretical expectation that the independent set approach (either implicit or explicit) is indeed worthwhile.

\section{Further research}

Even though in Section \ref{sec:conclusions_po} we noted how the algorithm proposed in \cite{mediumgrain} is still the most efficient for the initial split into $A_r$ and $A_c$, it might still be convenient to investigate additional strategies and compare their efficiency, in order to find a better method or gain additional confidence on this algorithm.

The partition-aware heuristics proposed in this thesis, while marginally effective, are meant as a first attempt at a fully iterative approach at sparse matrix partitioning, and further research can easily be performed. It might be wortwhile, for example, to investigate more thoroughly the properties of the maximum independent set, in order to fully exploit the bidimensionality of the medium-grain model.

Even though the implementation of the Hopcroft-Karp algorithm described in Section \ref{sec:hopcroft_karp} was successful in most cases (and the computation time required was, in general, reasonable), it might be interesting to produce a C implementation, to allow an eventual integration with the Mondriaan software package. 

In addition, if further research on our findings with respect to rectangular matrices is successful (i.e. our results are consistently reproducible), we could add our strategy as an optional feature to the partitioner: the program, before applying any other technique, might ask the user whether he/she intends to sacrifice computation time for a better partitioning. If so, the software would try to recognize whether the considered matrix is strongly rectangular and eventually execute our approach.
