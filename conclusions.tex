\chapter{Conclusions and further developments} \label{chap:conclusions}

The goal of this thesis was firstly to investigate the possibility of performing sparse matrix partitioning in an iterative fashion, devising a systematic approach for deciding which information on the previous partitioning should be kept for an improvement on the quality of the solution, and what can be safely discarded. Secondly, our intent was also to explore the margins of improvement of the initial solution using the medium-grain model.

We were able to deal with these two research directions at the same time, applying basic principles to come up with simple schemes that could serve either purpose (with minor modifications). In the following sections we summarize our findings and try to envision possibile further developments in these directions.

\section{Improvement of the initial solution} \label{sec:conclusions_po}

As already outlined in Section \ref{sec:mediumgrain}, for the medium-grain model the initial split of the matrix $A$ into $A_r$ and $A_c$ is of vital importance: our goal was to investigate whether the algorithm proposed by the authors in \cite{mediumgrain} was indeed a good choice and to what extent.

To this purpose, we devised a number of different algorithms in Section \ref{sec:localview}, \ref{sec:hot_restart} and \ref{sec:is_vector} which exploited different features of the considered matrix. Among the proposed partition-oblivious heuristics, \verb|po_localview| turned out to be the best, but not quite reaching the same level of efficiency of \verb|medium-grain|.

Keeping in mind that \verb|po_localview| is none other than a simplified version of such \verb|medium-grain| heuristic, this result is interesting from two point of views. Firstly, as this scheme does not include the iterative refinement procedure, briefly outlined in Section \ref{sec:preliminary_po}, it is not surprising that the final communication value is on average slightly worse (7\%) than the original heuristic. Secondly, as this method was still the best performing among the ones devised in this thesis, we have an additional motivation for the validity of the algorithm proposed originally along with the model.

\section{Iterative partitioning} \label{sec:conclusions_pa}

It is reasonable to assume that the initial split of the matrix $A$ into $A_r$ and $A_c$, could be performed more efficiently with additional information, especially if the matrix $A$ has already been (bi)partitioned. 

The main feature of a partitioning we decided to preserve, was the fact that a row/column was uncut: this means that the partitioner decided, at some point in the previous iteration, that it was convenient for the nonzeros of such row/column to be assigned to the same processor. It is then reasonable to devise heuristics which tend to give a preference for these uncut rows/columns, trying to keep them uncut also in the next iteration. Naturally, as it is not always possible to do (first of all because we are not directly assigning to processors, but rather to $A_r$ and $A_c$), a more complex approach is usually needed.

To this extent, a wide variety of heuristic has been devised: the partition-aware extension of the original algorithm, along with the heuristics that employ the Separated Block Diagonal of order 1 and 2 of the partitioned matrix $A$, quickly turned out to be far from effective. The other considered algorithms, instead, yielded more interesting results.

Albeit we were not able to devise a single scheme that outperforms \verb|medium-grain| for all the matrices involved in the testing, the results with rectangular matrices were quite encouraging. Chapter \ref{chap:independent_set} was focused on the concept of independent set, and the experimental results suggest that this is an interesting approach: our best heuristics (\verb|pa_localbest|, \verb|pa_is_1|, \verb|pa_is_3|) rely indeed on this concept, implicitly or explicitly. Even the simple concatenation of rows and columns in the priority vector $v$ can be intended as a special case of the ideas of Section \ref{sec:is_vector}: the set of rows (or columns) is as a matter of fact an independent set, albeit probably not of maximum cardinality. The improvement of the results of this heuristic with increasing rectangularity of the matrices, is also to be seen in this perspective: the more one dimension is dominant, the more that set of indices has a cardinality closer to the maximum independent set, when computed on the graph constructed according to Section \ref{sec:is_graph}.

The experimental results confirmed our theoretical expectation that the indipendent set approach is indeed worthwile: considering only the rectangular matrices in our test bed, the results of these heuristics were better (albeit marginally) than \verb|medium-grain|.

\section{Further research}

Even though in Section \ref{sec:conclusions_po} we noted how the algorithm proposed in \cite{mediumgrain} is still the most efficient for splitting a matrix $A$ into $A_r$ and $A_c$ for the medium-grain model, after comparing it with many other different heuristics, it might still be convenient to research additional strategies, in order to find a more efficient method or gain additional confidence on this heuristic.

The partition-aware heuristics proposed in this thesis are meant as a first attempt at a fully iterative approach at sparse matrix partitioning, and further research can easily be performed. It might be wortwhile, for example, to investigate more thoroughly the properties of the maximum independent set, in order to fully exploit the bidimensionality of the medium-grain model.

In addition, the iterative refinement procedure described in \cite{mediumgrain} might be extended also to our results, especially for the partition-aware heuristics.

Even though the implementation of the Hopcroft-Karp algorithm described in Section \ref{sec:hopcroft_karp} was successful in most cases (and it was performed in a reasonable amount of time even for the bigger matrices), it might still be wortwhile to produce a C implementation, in order to allow the eventual integration with the Mondriaan software package. 

In addition, if further research on our findings with respect to rectangular matrices is successful (i.e. we are able to get good partitioning on such matrices in a consistent manner), it could be added as an optional feature to the partitioner: the program, before applying any other technique, might ask the user whether he intends to sacrifice computation time for a better partitioning. If so, the software would try to recognize whether the considered matrix is strongly rectangular and eventually execute our approach.

