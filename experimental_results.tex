\chapter{Implementation and experimental results} \label{chap:experimental_results}

In Chapter \ref{chap:methods} and \ref{chap:independent_set} we discussed different heuristics aimed at solving the the matrix partitioning problem; whether we try to improve the initial partitioning or perform a fully iterative procedure, we need to translate those ideas into practice, devising an efficient implementation.

In Section \ref{sec:earlier_work}, we mentioned existing software partitioners: Mondriaan \cite{mondriaan} is the package of our choice and we defer to it the actual computations of the partitionings, limiting ourselves to create the matrix $B$ of the medium-grain model as in \eqref{eq:Bmatrix}. The actual algorithm used to construct this matrix is given explicitly in Algorithm \ref{alg:B}. Note how we consider only the sparsity patterns of $A$, neglecting completely the values of the nonzeros.

\begin{algorithm}[h]
	\begin{algorithmic}
		\Require{$A_r$, $A_c$}
		\Ensure{$B$}
		\State $B \gets \varnothing$
		\ForAll{$a_{ij} \in A_c$} \Comment{The part relative to $A_c$}
		\State $b_{i+n,j} = 1$
		\EndFor
		\ForAll{$a_{ij} \in A_r$} \Comment{The part relative to $A_r$}
		\State $b_{j,i+n} = 1$
		\EndFor
		\For{$j=1,\dots,n$} \Comment{Dummy nonzeros for cut columns}
		\If{$\exists\, i$ s.t. $a_{ij} \in A_r$ and $\exists\, i'$ s.t. $a_{i'j} \in A_c$}
		\State $b_{i,i} = 1$
		\EndIf
		\EndFor
		\For{$i=1,\dots,m$} \Comment{Dummy nonzeros for cut rows}
		\If{$\exists\, j$ s.t. $a_{ij} \in A_r$ and $\exists\, j$ s.t. $a_{ij'} \in A_c$}
		\State $b_{n+i,n+i} = 1$
		\EndIf
		\EndFor
	\end{algorithmic}
	\caption{Construction of $B$ following the medium-grain model.} \label{alg:B}
\end{algorithm}

Now, having discussed the means of obtaining $A_r$ and $A_c$ and the matrix $B$, we can outline the general framework used to test the effectiveness of the proposed heuristics. The framework is given explicitly in Algorithm \ref{alg:framework}, and it takes as a parameter the maximum number of iterations allowed, $iter_{max}$.  

\begin{algorithm}[h]
	\begin{algorithmic}
		\Require{Sparse matrix $A$}
		\Ensure{Partitioning for the matrix $A$}
		\State Partition $A$ with Mondriaan using the default options and the medium-grain model
		\For{$i=0,\dots,iter_{max}$}
		\State Use any of the heuristics described previously to compute $A_r$ and $A_c$
		\State compute $B$, using Algorithm \ref{alg:B}, from $A_r$ and $A_c$
		\State Partition $B$ with Mondriaan using the default options and the row-net model
		\State Re-construct $A$ with the new partitioning
		\EndFor
	\end{algorithmic}
	\caption{General framework for the testing of our heuristics} \label{alg:framework}
\end{algorithm}

In Chapter \ref{chap:methods} and \ref{chap:independent_set}, we distinguished between partition-oblivious and partition-aware methods, and Algorithm \ref{alg:framework} is suitable for both types of heuristics: even though the framework is naturally suited for developing a fully iterative scheme, if we are after a better initial partitioning for the medium-grain, we can simply neglect the partitioning done in the first step. This is precisely the scope of a partition-oblivious heuristic.

Regarding the actual implementation, we can see from Algorithm \ref{alg:framework} that Mondriaan is used to perform the actual partitioning and this is the ideal case for its use as a software library. As a consequence, we used C as the main implementation language, even though MATLAB was used for faster prototyping: the flexibility added by managing objects at runtime  is ideal when designing algorithms. In order to have C code and MATLAB code interact in the correct way, we took advantage of MEX files \cite{mex}. In general, unless preliminary tests showed that the considered heuristic had a remarkably bad quality, we translated back everything to the C language, in order to remove the MEX layer of complexity and get a more efficient implementation. For the Hopcroft-Karp algorithm described in Chapter \ref{chap:independent_set}, we used an implementation \cite{hkarp_impl} written in the python programming language which computes directly the matching on a bipartite graph and the maximum independent set. 

The parameter $iter_{max}$ of Algorithm \ref{alg:framework} is of vital importance for the running time of our iterative method: it is true that our research motivation is to trade computation time for solution quality, but efficiency in doing so is a fundamental goal. Therefore, an iterative scheme that improves the quality of the solution very slowly (thus needing a high $iter_{max}$ to improve as much as possible) is not desirable; fortunately, preliminary tests showed that our heuristics are indeed very fast in this aspect, and only one iteration is needed: additional ones either improve the solution quality by a very small amount (which likely depends on the probabilistic nature of the heuristic and the partitioner) or produce the opposite effect, resulting in a much worse solution. Table \ref{tab:iterations} shows an example of multiple iterations for some of the described heuristic.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c||c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{\textbf{Heuristic}} & \multicolumn{11}{c|}{\textbf{Iterations}} \\ \cline{2-12} 
		& 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
		\verb|po_localview| & 599 & 575 & 576 & 578 & 573 & 583 & 578 & 574 & 575 & 578 & 573 \\
		\verb|pa_localview| & 588 & 577 & 580 & 577 & 577 & 574 & 578 & 580 & 576 & 579 & 575  \\
		\verb|sbdview| & 590 & 588 & 591 & 579 & 597 & 595 & 589 & 603 & 595 & 589 & 587 \\
		\verb|po_unsorted_concat| & 579 & 597 & 594 & 593 & 593 & 597 & 597 & 598 & 594 & 603 & 596 \\ 
		\hline
	\end{tabular}
	\caption{Multiple iterations of four different heuristics for the test matrix \texttt{dfl001}. The numbers shown are the rounded arithmetic means of each iteration over 10 repeats of the experiment. Iteration 0 stands for the initial partitioning obtained with the medium-grain model. } \label{tab:iterations}
\end{table}

In order to perform effective numerical experiments, we need to have a consistent way of testing. First of all, since we are after heuristics suitable for many matrices, it makes sense to have several matrices to test for as described in Section \ref{sec:test_matrices}. 

Secondly, since randomness is involved in the partitioner itself and, to a different extent, in some of the considered heuristics, in order to have insightful results we need to perform several measurements and compute an average. After generating an initial partitioning, one iteration of the heuristic was performed independently 5 times, and their result was averaged; this was repeated 20 times, obtaining an average of the 20 initial partitioning and an average of the 20 averages of final results. 

Lastly, in order to have a meaningful result that can help us understand whether the given heuristic is globally effective, we will compute the geometric mean of all the initial partitioning and the geometric mean of all the final results. Then we will normalize w.r.t the first value and understand whether the considered heuristic performs better, on average, or not. This normalized geometric mean is denoted, in the next tables, with the symbol $\rho$.

\section{Test matrices} \label{sec:test_matrices}

In order to have insightful results, we mentioned that the matrices used in the numerical experiments should have different features: in particular we distinguish between  (strongly) rectangular matrices and square matrices, and try to have a wide selection w.r.t. the number of nonzeros. 

These matrices are mainly from the University of Florida Sparse Matrix Collection \cite{ufl}, and some can additionally be found on the Matrix Market collection \cite{matrixmarket} and Table \ref{tab:matrices} provides a more thorough description of the matrices used, along with an outline of their basic properties (number of rows $m$, number of columns $n$, number of nonzeros $N$) and their original purpose. Some of these matrices (namely the ones with the $\dagger$ symbol in the table) belong to the 10th Dimacs Implementation Challenge \cite{dimacs}, which addressed the graph partitioning and graph clustering problem, and are therefore naturally suited for testing the quality solutions of our algorithms.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|r|r|r|p{7cm} |}
		\hline	
		\textbf{Name} & \textbf{$m$} & \textbf{$n$} & \textbf{$N$} & \textbf{Source problem} \\ \hline
		\verb|lpi_ceria3d| 									& 3576 		& 4400 		& 21178 & Netlib Linear Programming \\
		\verb|dfl001| 											& 12230 	& 6071 		& 35632 & Netlib Linear Programming \\ 
		\verb|delaunay_n15| $\dagger$ 			& 32768 	& 32768 	& 196548 & Delaunay triangulations of random points in plane \\ 
		\verb|deltaX| 											& 68600 	& 21961 	& 247424 & High fillin with exact partial pivoting \\
		\verb|cre_b| 												& 9648 		& 77137 	& 260785 & Netlib Linear Programming \\ 
		\verb|tbdmatlab| 										& 19859 	& 5979 		& 430171 & Term-by-document matrix \\ 
		\verb|nug30| 												& 52260 	& 379350 	& 1567800 & Netlib Linear Programming \\ 
		\verb|coAuthorsCiteseer| $\dagger$ 	& 227320 	& 227320 	& 1628268 & Citation and coauthor network\\ 
		\verb|bcsstk32| $\dagger$ 					& 44609 	& 44609 	& 2014701 & Stiffness matrix for automobile chassis \\ 
		\verb|bcsstk30| $\dagger$						& 28924 	& 28924 	& 2043492 & Stiffness matrix for off-shore generator platform \\
		\verb|c98a| 												& 56243 	& 56274 	& 2075889 & Factorization of composite integers with 98 decimal digits  \\ 
		\verb|wave| 	$\dagger$							& 156317 	& 156317 	& 2118662 & 3D finite elements \\
		\verb|tbdlinux| 										& 112757 	& 20167 	& 2157675 & Term-by-document matrix \\
		\verb|stanford| 										& 281903 	& 281903 	& 2312497 & Links between pages in Stanford website \\
		\verb|rgg_n_2_18_s0| $\dagger$ 			& 262144 	& 262144 	& 3094566 & Random graph \\
		\verb|polyDFT| 											& 46176 	& 46176 	& 3690048 & Polymer self-assembly \\ 
		\verb|cage13| 											& 445315 	& 445315 	& 7479343 & DNA Electrophoresis \\
		\verb|stanford_berkeley| 						& 683446 	& 683446 	& 7583376 & Links between Stanford and Berkeley websites \\
		\hline
	\end{tabular}
	\caption{Matrices used in our experiments.} \label{tab:matrices}
\end{table}

\section{Preliminary selection of the best heuristics}

Since in Chapter \ref{chap:methods} and \ref{chap:independent_set} we discussed many heuristics, which in turn depend on different parameters, it is best to perform a preliminary analysis to quickly figure out which produce the best solutions and should therefore be tested exenstively, and the ones that should not be furtherly considered.

For this reason, a small number of different matrices (with a relatively small amount of nonzeros) has been selected from our choice of Table \ref{tab:matrices}: in particular the considered matrices, which have a different structure, are \verb|dfl001|, \verb|tbdlinux|, \verb|nug30|, \verb|rgg_n_2_18_s0|, \verb|bcsstk30|.

The local search heuristic given in Section \ref{sec:globalview}, quickly turned out to be far from effective, and therefore we decided to discard it altogether in the numerical experiments. 

\subsection{Partition-oblivious heuristics}

Table \ref{tab:preliminary_po} summarizes the results of this preliminary analysis of the partition-oblivious heuristics for the 5 chosen matrices. In the first line, the results of the medium-grain with the algorithm proposed by the authors are given. The value $\rho$ represents the geometric mean of the results for a given heuristics, averaged over all matrices and normalized w.r.t. the default medium-grain.

\begin{table}[h]
	\centering

	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{|l||c|c|c|c|c||c|}
		\hline
		\multirow{2}{*}{\textbf{Heuristic}} &  \multicolumn{5}{|c||}{\textbf{Matrix}} & \multirow{2}{*}{$\rho$} \\ \cline{2-6}
		& \texttt{dfl001} & \texttt{nug\_30} & \texttt{bcsstk30} & \texttt{tbdlinux} & \texttt{rgg\_n\_2\_18\_s0} & \\ \hline
		medium-grain & 590 & 36262 & 552 & 8135 & 910 & 1.0 \\ \hline %2445
		\verb|po_localview|& \textbf{571} & 36665 & \textbf{598} & 8327 & 1160  & 1.07 \\  %2609.2
		\verb|po_unsorted_concat_row|& 1492 & 189689 & 653 & 15081 & 1098 & 2.04 \\ % 0 0 0 %4979.1
		\verb|po_unsorted_concat_col|& 589 & 38491  & 600 & 24024 & \textbf{1066} & 1.32 \\ % 0 1 0 %3224.1
		\verb|po_unsorted_random|& 1314 & 113070 & 1127 & 20154 & 1093 & 2.11 \\  % 2 0 0 %5169
		\verb|po_unsorted_mix_alt|& 1461 & 181216 & 715 & 27942 & 1104 & 2.32 \\  % 4 0 0 % 5666
		\verb|po_unsorted_mix_spr|& 1322 & 81915 & 759 & 18584 & 1122 & 1.81 \\  % 4 1 0 % 4434
		\verb|pa_sorted_w_simple|& 597 & 38383 & 785  & 8307 & 1093 & 1.13 \\ % 6 0 0 % 2771
		\verb|po_sorted_nw_simple|& 606 & 38674 & 789 & \textbf{8301} & 1096 & 1.14 \\ % 6 1 0 % 2787
		\verb|po_sorted_w_concat_row|& 1486 & 189681 & 642 & 15082 & 1078 & 2.01 \\ % 8 1 0 % 4907
		\verb|po_sorted_w_concat_col|& 597 & 38655 & 621 & 24045 & 1068 & 1.33 \\  % 8 1 1 % 3261
		\verb|po_sorted_nw_concat_row|& 1496 & 189683 & 614 & 15086 & 1090 & 2.01 \\ % 8 0 0 % 4913
		\verb|po_sorted_nw_concat_col|& 593 & 38513 & 621 & 24005 & 1076 & 1.33 \\ % 8 0 1 % 3256
		\verb|po_sorted_w_mix_alt|& 1317 & 162549 & 790 & 23683 & 1091 & 2.19 \\ % 10 0 1 % 5346
		\verb|po_sorted_w_mix_spr|& 641 & 163013 & 782 & 23441 & 1093 & 1.88 \\ % 10 0 0 % 4615
		\verb|po_sorted_nw_mix_alt|& 1457 & 62273 & 797 & 15015 & 1096 & 1.69 \\ % 10 1 0  % 4122
		\verb|po_sorted_nw_mix_spr|& 719 & 62402 & 793 & 15072 & 1106 & 1.47 \\  % 10 1 1 % 3586
		\verb|po_is|& 594 & \textbf{30655} & 615 & 13286 & -  & 1.12 \\ % 3491 
		\hline
	\end{tabular}
	\caption{Results of the devised partition-oblivious heuristics for the five chosen matrices. In each column, we use the boldface to highlight the best found partitioning for each matrix (not considering the medium-grain value). } \label{tab:preliminary_po}
\end{table}

In table there is no result for the heuristic \verb|po_is| and the  matrix \verb|rgg_n_2_18_s0|, because the maximum number of levels of recursion was reached before the algorithm completed.

\subsection{Partition-aware heuristics}

In Table \ref{tab:preliminary_pa} we summarize the results of the partition-aware heuristics. As now we are considering a fully iterative framework, it is best to explicitly give the average of the 20 initial partitionings (iteration 0) alongside the average of the 20 final results (iteration 1). The value $\rho$ represents, similarly as before, the geometric mean of the final results, normalized w.r.t. the average of the initial partitionings for that heuristic. 

\begin{table}[h]
	\centering
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{|l|c||c|c|c|c|c||c|}
		\hline
		\multirow{2}{*}{\textbf{Heuristic}} & \multirow{2}{*}{$i$} &  \multicolumn{5}{|c||}{\textbf{Matrix}} & \multirow{2}{*}{$\rho$} \\ \cline{3-7}
		& & \texttt{dfl001} & \texttt{nug\_30} & \texttt{bcsstk30} & \texttt{tbdlinux} & \texttt{rgg\_n\_2\_18\_s0} & \\ \hline
		\verb|pa_localview| & 0 & 582 & 36224 & 537 & 8051 & 914  \\ % 2422
		& 1 & \textbf{575} & 36896 & 577 & 9934 & 2189 \\ \hline %3055
		\verb|pa_sbdview| & 0 & 590 & 36057 & 546 & 8112 & 899 \\ % 2430
		& 1 & 1493 & 187241 & 699 & 19852 & 1074 \\ \hline % 5296
	\verb|pa_sbd2view| & 0 & 583 &  36123 & 542 & 8018 & 906 \\
		& 1 & 1276 & 125009 & 1150 & 20757 & 1055\\ \hline
		\verb|pa_unsorted_concat_row| & 0 & 584 & 36512 & 597 & 7934 & 929 \\ % 1 0 * 
		& 1 & 628 & 42581 & 641 & 7337 & 1088 \\\hline
		\verb|pa_unsorted_concat_col| & 0 & 589 & 35945 & 574 & 7999 & 898 \\ % 1 1 *
		& 1 & 589 & 38862 & 602 & 10087 & 1069 \\\hline
		\verb|pa_unsorted_random| & 0 & 591 & 36390 & 550 & 8044 & 909 \\ % 3 * *
		& 1 & 622 & 36952 & 589 & 8383 & 1094 \\ \hline
		\verb|pa_unsorted_mix_alt| & 0 & 592 & 36097 & 536 & 8019 & 896 \\ % 5 0 *
		& 1 &  633 & 40286 & 684 & 9683 & 1108 \\ \hline
		\verb|pa_unsorted_mix_spr| & 0 & 589 & 36137 & 542 & 8024 & 905 \\ % 5 1 *
		& 1 &  642 & 39703 & 645 & 8361 & 1094 \\ \hline
		\verb|pa_sorted_w_simple| & 0 & 594 & 36589 & 544 & 8012 & 876 \\ % 7 1 *
		& 1 &  599 & 38887 & 563 & 8002 & 1093 \\ \hline
		\verb|pa_sorted_nw_simple| & 0 & 596 & 36115 & 542 & 7995 & 888 \\ % 7 0 *
		& 1 &  599 & 38793 & 572 & 8014 & 1098 \\ \hline
		\verb|pa_sorted_w_concat_row| & 0 & 585 & 36305 & 537 & 8006 & 898 \\ % 9 1 0 
		& 1 &  1494 & 190799 & 661 & 19036 & 1087 \\ \hline
		\verb|pa_sorted_w_concat_col| & 0 & 590 & 36164 & 537 & 8032 & 903 \\ % 9 1 1
		& 1 &  954 & 115312 & 689 & 25157 & 1074 \\ \hline
		\verb|pa_sorted_nw_concat_row| & 0 & 590 & 35955 & 545 & 7999 & 897 \\ % 9 0 0
		& 1 & 1489 & 190657 & 693 & 19152 & 1073 \\ \hline
		\verb|pa_sorted_nw_concat_col| & 0  & 597 & 36436 & 548 & 8009 & 891 \\ % 9 0 1
		& 1 & 975 & 116063 & 707 & 25543  & 1063 \\ \hline
		\verb|pa_sorted_w_mix_alt| & 0 &  586 & 36566 & 549 & 8006  & 912 \\ % 11 0 1
		& 1 & 692 & 41963 & 593 & 9476 & 1105 \\ \hline
		\verb|pa_sorted_w_mix_spr| & 0 &  593 & 36085 &  537 & 8010 & 924 \\ % 11 1 1
		& 1 & 619 & 39697 & 571 &  9060 & 1078 \\ \hline
		\verb|pa_sorted_nw_mix_alt| & 0 & 588 & 36509 & 546 & 8020 & 891 \\ % 11 0 0
		& 1 &  687 & 42542 & 566 & 9486 & 1085 \\ \hline
		\verb|pa_sorted_nw_mix_spr| & 0 & 589 & 36197 & 553 &8006  & 886 \\ % 11 1 0
		& 1 &  655 & 39201 & 602 & 9076 & 1094 \\ \hline
		\verb|pa_is_1| & 0 & 592 & 36716 & 534 & 7997 & \\ 
		& 1 & 588 & \textbf{36118} & 614 & \textbf{7323} & - \\ \hline
		\verb|pa_is_2| & 0 & 596 & 35972 &  539 & 8011 &  \\ 
		& 1 & 592 & 36375 &  631 & 8681 & - \\ \hline
		\verb|pa_is_3| & 0 & 590 & 36432 & 540 & &\\ 
		& 1 & 589 & 36324 & 635 & & -\\ \hline

	\end{tabular}
	\caption{Results of the devised partition-oblivious heuristics for the five chosen matrices. In each column, we use the boldface to highlight the best found partitioning for each matrix. } \label{tab:preliminary_pa}
\end{table}

Similarly as before, the computation of the independent set was not possible in the case of the matrix \verb|rgg_n_2_18_s0| because the maximum levels of recursion was reached before being able to finish the algorithm.

\subsection{Observations on the preliminary tests}

These preliminary tests proved themselves useful, as we could effectively narrow down the best heuristics among the ones proposed; moreover, as a side effect, we are able to make a few general observations regarding desirable and undesirable features of the heuristics.

First of all, we can see that in Table \ref{tab:preliminary_po} the first heuristic, discussed in Section \ref{sec:localview}, is the one that produced the best results among the partition-oblivious ones. This is not surprising, as the algorithm is very similar to the one originally proposed by the authors in \cite{mediumgrain}, even though without a few refinements, which result in our \verb|po_localview| slightly worse than our reference values. The results with the partition-oblivious computation of the independent set was surprisingly good, especially with the matrix \verb|nug_30|.

Secondly, regarding the family of heuristics discussed in Section \ref{sec:hot_restart}, it appears that mixing rows and columns in in general not advisable: for both the partition-oblivious and partition-aware schemes, the results are considerably worse than the reference values. 

The same principle can be deducted also from the surprisingly good results of the \verb|unsorted_concat| family of schemes, in which there is a clear separation of rows and columns. For these schemes, it appears that the matrix structure (and in particular, how far it is from being square) heavily influence the quality of the final partitioning. Furthermore, they are mutually exclusive, as expected: if one produces good results, the other one performs terribly. The consistency of their behavior suggests us that they could be merged into a \verb|po_unsorted_concat| and \verb|pa_unsorted_concat| schemes that automatically choose whether having columns-rows or rows-columns, depending on the size of the matrix to be partitioned.

Moreover, it appears that moving or not the indices with one nonzero to the back (denoted, respectively, by \verb|w| and \verb|nw|) does not yield a substantial difference: the results of the first case are usually better, but not as much as expected. In both cases, however, sorting w.r.t the number of nonzeros does not seem to yield a dramatic improvement over the unsorted heuristics.

\section{Analysis of the performance of the best heuristics through the test matrices}



